# -*- coding: utf-8 -*-
"""Advanced Product Analytics & A/B Testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mst1-Pxjy1qvZXbbbUnEa9yJCjloGQPv

Context: We are Data Scientist at "MusicStream."\
Goal: Prove the value of a recommendation algorithm using rigorous statistical methods.\
Tools: Python (Pandas, Plotly, SciPy, Lifelines).
"""

pip install faker plotly lifelines scipy numpy pandas

pip install plotly statsmodels

import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from scipy import stats
from statsmodels.stats.power import TTestIndPower
from lifelines import KaplanMeierFitter
from faker import Faker
from statsmodels.stats.proportion import proportions_ztest

# Configuration for reproducibility
np.random.seed(42)
fake = Faker()

"""***Phase 1 - Power Analysis***

We calculate how many users we need. This prevents "Underpowered" tests.
"""

# Experiment Parameters
effect_size = 0.05   # Small effect (Cohen's d)
alpha = 0.05         # 5% significance level
power = 0.80         # 80% power

# Unbalanced Split (90% Control / 10% Treatment)
# Ratio = Treatment Size / Control Size
treatment_ratio = 0.10 / 0.90

analysis = TTestIndPower()
# Solve for Control Group Size
n_control = analysis.solve_power(effect_size=effect_size, power=power, alpha=alpha, ratio=treatment_ratio)
n_treatment = n_control * treatment_ratio

total_required = int(n_control + n_treatment)

print(f"--- Experiment Design (90/10 Ramp-Up) ---")
print(f"Control Users Needed:   {int(n_control):,}")
print(f"Treatment Users Needed: {int(n_treatment):,}")
print(f"Total Users Needed:     {total_required:,}")
print("\nInsight: Unbalanced tests require MORE total users than 50/50 tests to achieve the same statistical power.")

"""***Phase 1 - A/A Test Simulation (The Baseline Check)***

Before running the real test, we simulate 1,000 "null" tests to prove our stats engine doesn't generate false positives.
"""

p_values = []
# Run 1000 simulations
for _ in range(1000):
    # Generate two identical groups (No treatment effect)
    group_a = np.random.normal(20, 5, 1000)
    group_b = np.random.normal(20, 5, 1000)

    # Run T-Test
    _, p = stats.ttest_ind(group_a, group_b)
    p_values.append(p)

# Calculate False Positive Rate (Target: ~5%)
fp_rate = sum(x < 0.05 for x in p_values) / 1000

print(f"False Positive Rate: {fp_rate:.1%} (Target: 5.0%)")

if 0.04 <= fp_rate <= 0.06:
    print("System Calibrated: We are ready for the A/B test.")
else:
    print("Calibration Warning: Review randomization logic.")

"""***Phase 2 - Data Engineering***

This generates the Raw Data (Users + Events) AND aggregates into a table for analysis
"""

def generate_industry_data(n_users=40000):
    print(f"Generating raw logs for {n_users:,} users...")

    # --- 1. USERS TABLE ---
    groups = np.random.choice(['Control', 'Treatment'], size=n_users, p=[0.90, 0.10])

    df_users = pd.DataFrame({
        'user_id': range(n_users),
        'group': groups,
        'country': np.random.choice(['US', 'UK', 'DE', 'IN'], n_users, p=[0.4, 0.2, 0.2, 0.2]),
        'device': np.random.choice(['iOS', 'Android'], n_users, p=[0.5, 0.5]),
        'age_group': np.random.choice(['18-24', '25-34', '35-44', '45+'], n_users, p=[0.35, 0.35, 0.2, 0.1]) #skewed data as music customers are more young
    })

    # --- 2. EVENTS LOG ---
    activity_counts = np.zeros(n_users, dtype=int)
    mask_c = df_users['group'] == 'Control'
    mask_t = df_users['group'] == 'Treatment'

    # Treatment Effect: 15% Lift in Activity
    activity_counts[mask_c] = np.random.gamma(2, 10, size=mask_c.sum()).astype(int)
    activity_counts[mask_t] = np.random.gamma(2, 11.5, size=mask_t.sum()).astype(int)

    # ETL (Aggregation)
    df_users['songs_played'] = activity_counts

    # --- 3. CONVERSION DATA ---
    df_users['propensity'] = 0.05 + (df_users['songs_played'] * 0.002) ## base: conversion is based on songs played
    df_users.loc[df_users['country'] == 'US', 'propensity'] *= 1.2 ## bias: US people tend to spend more
    df_users['converted'] = np.random.binomial(1, df_users['propensity'].clip(upper=1.0))

    # --- 4. RETENTION DATA ---
    # Logic:
    # - Everyone starts with random tenure (1-60 days)
    # - 'Converted' users stay longer (Lower churn)
    # - 'Treatment' users stay slightly longer (Better experience)

    # Base tenure
    df_users['days_active'] = np.random.randint(1, 60, size=n_users)

    # Churn Probability Calculation
    # Base churn risk = 30%
    churn_risk = np.full(n_users, 0.30)

    # Premium users are much less likely to churn (Risk -20%)
    churn_risk[df_users['converted'] == 1] -= 0.20

    # Treatment group is slightly less likely to churn (Risk -5%)
    churn_risk[df_users['group'] == 'Treatment'] -= 0.05

    # Simulate Churn Event (1 = Churned/Left, 0 = Still Active/Censored)
    df_users['churned'] = np.random.binomial(1, churn_risk.clip(0, 1))

    # --- 5. INJECT OUTLIERS ---
    outlier_indices = np.random.choice(df_users.index, size=50, replace=False)
    df_users.loc[outlier_indices, 'songs_played'] = np.random.randint(1000, 5000, size=50)

    return df_users

# Generate the Master Dataset
df_raw = generate_industry_data()
display(df_raw.head())

"""***Phase 2 - Outlier Detection & Cleaning***

Visualizing and removing outliers.
"""

# 1. Visualize Boxplot
fig = px.box(df_raw, x='group', y='songs_played',
             title='Boxplot of Songs Played (Before Cleaning)',
             template='plotly_white')
fig.show()

# 2. Define Threshold (99th Percentile)
threshold = df_raw['songs_played'].quantile(0.99)
print(f"99th Percentile Threshold: {threshold:.0f} songs")

# 3. Filter Data
df_clean = df_raw[df_raw['songs_played'] < threshold].copy()
removed = len(df_raw) - len(df_clean)

print(f"Removed {removed} outliers (Bots/Whales).")
print(f"Cleaned Dataset Size: {len(df_clean):,}")

"""***Why we need to remove outliers?***

***Phase 3 - Guardrails (SRM Check)***

Validating the 90/10 split. If this fails, the experiment is broken.
"""

# Observed Counts
obs = df_clean['group'].value_counts()
# Order: [Control, Treatment] to match expectation logic below
obs_values = [obs['Control'], obs['Treatment']]

# Expected Counts (90% Control, 10% Treatment)
total_clean = len(df_clean)
exp_values = [total_clean * 0.90, total_clean * 0.10]

print(f"Observed: {obs_values}")
print(f"Expected: {exp_values}")

chi2, p_val = stats.chisquare(f_obs=obs_values, f_exp=exp_values)
print(f"P-Value: {p_val:.5f}")

if p_val < 0.01:
    print("SRM WARNING: The split is not 90/10!")
else:
    print("SRM PASSED: Groups are balanced correctly.")

"""***Why we do chisquare test?***\
It is a "Sanity Check" to ensure the experiment isn't broken.

1. The Logic: Expected vs. Observed

When we design an A/B test, we program it to split traffic 10/90.

If we have 10,000 users, we expect 9,000 in Control and 1,000 in Treatment.

However, in the real world, bugs happen. Maybe the new feature crashes on older Android phones, so those users never get logged into the "Treatment" group.

Is the user difference just random luck? Or is the randomization engine broken? The Chi-Square Test answers this question. It compares the numbers we Observed against the numbers we Expected.

2. How to Read the Result

High P-Value (> 0.01): The difference is small. It's likely just random noise. The experiment is VALID.

Low P-Value (< 0.01): The difference is huge. It is extremely unlikely to happen by chance. The experiment is BROKEN.

Why this matters: If our sample is mismatched (e.g., 55% Control / 45% Treatment), it often means a specific type of user (e.g., Chrome users, iPhone users) is failing to enter the experiment. If we analyze the data anyway, our results will be wrong.

***Phase 4 - Statistical Inference (T-Test)***

Analyzing the Primary Metric (Songs Played). Note the use of equal_var = False
"""

control = df_clean[df_clean['group']=='Control']['songs_played']
treat = df_clean[df_clean['group']=='Treatment']['songs_played']

# Welch's T-Test (equal_var=False)
t_stat, p_val = stats.ttest_ind(control, treat, equal_var=False)
lift = (treat.mean() - control.mean()) / control.mean()

print(f"Lift: {lift:.2%}")
print(f"P-Value: {p_val:.5f}")

if p_val < 0.05:
    print("RESULT: Statistically Significant Increase.")

"""***Why Welch's T-Test? (Instead of Student's T-Test)***

We use Welch's T-test because our experiment has Unequal Sample Sizes (90% vs 10%) and likely Unequal Variances.

The Problem: The standard "Student's T-Test" assumes that both groups have the exact same variance (spread).

The Reality: If our new algorithm works, it often makes power users listen to even more music, which increases the variance in the Treatment group.

The Risk: When we have unequal sample sizes (N=36k vs N=4k) AND unequal variance, the standard T-Test gives you a False Positive (it says the result is real when it isn't). Welch's T-test adjusts the "Degrees of Freedom" to account for this. It is the safe default in Tech.

***Phase 4 - Conversion Analysis (Z-Test)***

Analyzing the Secondary Metric (Purchase Rate)
"""

convs = df_clean.groupby('group')['converted'].sum()
nobs = df_clean.groupby('group')['converted'].count()

# Ensure order [Treatment, Control]
count = [convs['Treatment'], convs['Control']]
n_obs = [nobs['Treatment'], nobs['Control']]

stat, p_z = proportions_ztest(count, n_obs)

print(f"Treatment Conv Rate: {count[0]/n_obs[0]:.2%}")
print(f"Control Conv Rate:   {count[1]/n_obs[1]:.2%}")
print(f"P-Value: {p_z:.5f}")

"""***Why Z-Test for Conversion?***

We use the Z-Test because Conversion is a Proportion (Yes/No), not a continuous number.

T-Test is for Averages (e.g., "Average songs played is 24.5"). It handles data that can be 1, 5, 100, or 500.

Z-Test is for Rates (e.g., "Conversion Rate is 12%"). The data is strictly 0 or 1.

The Math: In a Z-test, the variance is strictly determined by the mean ($p(1-p)$). The Z-test for Proportions is specifically designed to handle the standard error of binary data.

***Result Interpretation***

We cannot prove that the new feature increases conversion. While the data looks directionally positive, the effect is too weak to be distinguished from random noise. We should NOT expect a guaranteed revenue increase if we launch.

***What if p-value is close to 0.05 but greater than 0.05?***

1. Explanation: What does a P-Value of 0.06 mean?
P-value of 0.06 is a very tricky result. It is often called "Directionally Positive but Not Statistically Significant."

The Math: It means there is a 6% chance that the improvement you see is just random luck.

The Standard: Most companies set the "Significance Level" (Alpha) at 0.05 (5%). Because 0.06 > 0.05, you technically failed the test. You cannot claim the feature definitely increases revenue.

Business Interpretation:

"We are 94% sure this feature helps, but our safety standard requires us to be 95% sure. If we launch this, there is a moderate risk (6%) that the revenue boost isn't real."

What would we do in a real job?

Do not launch yet.

Run the test longer: We might just need more sample size to push that 0.06 down to 0.04.

Check Segmentation: Maybe it is significant (p < 0.05) for iOS users but not Android. Launch only for iOS.

***Phase 5 - Deep Dive (Segmentation)***

Checking for Heterogeneous Effects

e.g. Does it fail on Android?
"""

# Calculate Lift by Device
seg = df_clean.groupby(['device', 'group'])['songs_played'].mean().unstack()
seg['Lift'] = (seg['Treatment'] - seg['Control']) / seg['Control']

display(seg)

# Plot
fig = px.bar(seg.reset_index(), x='device', y='Lift',
             title='Treatment Lift by Device (Segmentation)',
             text_auto='.1%', color='Lift', template='plotly_white')
fig.show()

"""Investigating if the feature works differently for different Age Groups."""

# Group by Age and Experiment Group
segmentation = df_clean.groupby(['age_group', 'group'])['songs_played'].mean().unstack()

# Calculate Lift per Segment
segmentation['Lift'] = (segmentation['Treatment'] - segmentation['Control']) / segmentation['Control']
segmentation['Lift %'] = segmentation['Lift'].apply(lambda x: f"{x*100:.2f}%")

display(segmentation[['Control', 'Treatment', 'Lift %']])

# Visualizing the Lift by Age
fig = px.bar(segmentation.reset_index(), x='age_group', y='Lift',
             title='Impact of New Algorithm by Age Group',
             labels={'Lift': 'Relative Lift in Songs Played'},
             color='Lift', color_continuous_scale='RdBu',
             template='plotly_white')
fig.show()

"""***Phase 6 - Visualization (Sankey Diagram)***

Visualizing the User Flow for the Treatment Group.
"""

# 1. Filter Data for Treatment Group Only
df_treat = df_clean[df_clean['group'] == 'Treatment']

# 2. Calculate Funnel Counts
total_users = len(df_treat)
# Users who did ANY activity (Songs > 0)
active_users = len(df_treat[df_treat['songs_played'] > 0])
# Users who paid
premium_users = df_treat['converted'].sum()

# Derived Metrics (Drop-offs)
bounced_users = total_users - active_users       # Users with 0 songs
free_users = active_users - premium_users        # Active but didn't pay

# 3. Define Nodes and Colors
# Indices: 0=Start, 1=Active, 2=Premium, 3=Bounce, 4=Free
labels = ["App Open", "Active (Played Songs)", "Premium Purchase", "Bounced (0 Songs)", "Remained Free"]
colors = ["blue", "blue", "green", "red", "gray"]

# 4. Define Links (Source -> Target)
source = [0, 0, 1, 1]
target = [1, 3, 2, 4]
values = [
    active_users,    # Open -> Active
    bounced_users,   # Open -> Bounce
    premium_users,   # Active -> Premium
    free_users       # Active -> Free
]

# 5. Plot
fig = go.Figure(data=[go.Sankey(
    node = dict(
      pad = 20, thickness = 20,
      line = dict(color = "black", width = 0.5),
      label = labels,
      color = colors
    ),
    link = dict(
      source = source,
      target = target,
      value = values,
      color = ["rgba(0,0,255,0.3)", "rgba(255,0,0,0.3)", "rgba(0,128,0,0.5)", "rgba(128,128,128,0.3)"]
    ))])

fig.update_layout(title_text=f"User Conversion Flow (Treatment Group, N={total_users})", font_size=12)
fig.show()

"""**Survival Analysis (Retention)**\
Visualizing how long users stick around using Kaplan-Meier curves.
"""

kmf = KaplanMeierFitter()
fig = go.Figure()

# 1. Fit Control Group
# We filter our cleaned data for Control users
data_control = df_clean[df_clean['group'] == 'Control']

kmf.fit(
    durations=data_control['days_active'],
    event_observed=data_control['churned'],
    label='Control'
)

# Get the probability data
surv_control = kmf.survival_function_
fig.add_trace(go.Scatter(
    x=surv_control.index, y=surv_control['Control'],
    mode='lines', name='Control Group', line=dict(color='blue')
))

# 2. Fit Treatment Group
data_treat = df_clean[df_clean['group'] == 'Treatment']

kmf.fit(
    durations=data_treat['days_active'],
    event_observed=data_treat['churned'],
    label='Treatment'
)

surv_treat = kmf.survival_function_
fig.add_trace(go.Scatter(
    x=surv_treat.index, y=surv_treat['Treatment'],
    mode='lines', name='Treatment Group', line=dict(color='orange', dash='dash')
))

# 3. Formatting
fig.update_layout(
    title="Kaplan-Meier Survival Curve (User Retention)",
    xaxis_title="Days Since Signup",
    yaxis_title="Probability of Remaining Active",
    template="plotly_white",
    hovermode="x unified"
)

fig.show()

print("Interpretation: If the Orange line is higher than the Blue line, Treatment users stay longer.")

"""**What's the summary of the current stage?**

We face the engagement - monetization gap.

When Engagement is up but Revenue is flat, usually one of three things is happening:

1. The "Free Ride" Problem: The new feature is too good for free users. They are so happy with the free product that they feel no need to upgrade. We cannibalized your own upsell.

2. The "Power Issue" (False Negative): Revenue events are rare (only 5-10% convert). We might simply lack the sample size to detect a small +2% revenue lift.

3. The "Wrong Segment" Problem: The feature works great for teenagers (who have no money) but fails for adults (who do).

**Next Steps**

**Analysis 1: Segmentation**

Hypothesis: Maybe the revenue lift is significant on a specific segment. If so, the global average washes out the win.

Action: Deep dive and see where is the money
"""

# Define segments to check
segments = ['device', 'country', 'age_group']

for segment in segments:
    print(f"\nAnalyzing by {segment.upper()}:")

    # Get unique values (e.g., 'iOS', 'Android')
    groups = df_clean[segment].unique()

    for g in groups:
        # Filter for just this sub-group
        subset = df_clean[df_clean[segment] == g]

        # Run Z-Test on this subset
        convs = subset.groupby('group')['converted'].sum()
        nobs = subset.groupby('group')['converted'].count()

        # Safety check: Need at least 10 conversions to run stats
        if convs.min() < 10:
            print(f"  - {g}: Not enough data.")
            continue

        stat, p = proportions_ztest([convs['Treatment'], convs['Control']],
                                    [nobs['Treatment'], nobs['Control']])

        lift = (convs['Treatment']/nobs['Treatment']) / (convs['Control']/nobs['Control']) - 1

        # Highlight significant findings
        icon = "✅" if p < 0.05 else "❌"
        print(f"  {icon} {g:<10} | Lift: {lift:+.2%} | P-Value: {p:.4f}")

"""**Analysis 2: Proxy Metric Analysis**

Hypothesis: Revenue is a "Lagging Indicator" (it takes 30 days to decide to pay). Engagement is a "Leading Indicator."

Action: Check "Add to Playlist" or "Share Song" rates. If these high-intent actions are up, revenue will follow eventually (LTV).
"""

# 1. Define "Power Users" (Top 25% of listeners)
threshold = df_clean['songs_played'].quantile(0.75)
power_users = df_clean[df_clean['songs_played'] >= threshold]

# 2. Check Conversion Lift specifically for Power Users
convs = power_users.groupby('group')['converted'].sum()
nobs = power_users.groupby('group')['converted'].count()

stat, p = proportions_ztest([convs['Treatment'], convs['Control']],
                            [nobs['Treatment'], nobs['Control']])

rate_t = convs['Treatment'] / nobs['Treatment']
rate_c = convs['Control'] / nobs['Control']
lift = (rate_t - rate_c) / rate_c

print(f"Power User Conv Rate (Control):   {rate_c:.2%}")
print(f"Power User Conv Rate (Treatment): {rate_t:.2%}")
print(f"Lift: {lift:+.2%}")
print(f"P-Value: {p:.4f}")

if p < 0.05:
    print("SIGNAL FOUND: The feature works for Power Users!")
    print("Recommendation: Launch. These users drive long-term value.")
else:
    print("Still not significant.")

"""**Analysis 3: Cannibalization Check**

Hypothesis: Did the treatment group view fewer "Upgrade Now" paywalls because the algorithm kept them listening smoothly?

Action: Check the number of paywall impressions.

While this didn't drive immediate upgrades, the 13% lift in listening time increases our Retention. Higher retention historically leads to higher Lifetime Value (LTV). We accept the flat revenue now to gain loyalty. (depends on the feature launching goal & product goal)

In the real world, data is rarely black and white. If Engagement is GREEN and Revenue is GRAY, we usually launch—because keeping users happy (Engagement) is the best way to make money in the long run. (depends on product manager and leadership business decision)
"""

